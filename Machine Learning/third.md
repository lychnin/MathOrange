[toc]
## 线性模型
线性模型试图学习一个通过属性的线性组合来进行预测的函数
$ f(x)=w_1x_1+w_2x_2+……+w_dx_d+b$
w与b学得之后，模型就得以确定
由于w直观表达了各属性在预测中的重要性，因此线性模型有很好的的可解释性

### 线性回归
线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记

若属性值间存在“序”关系，可以通过连续化将其转化为连续值
==“序关系是指有排序关系的关系，比如说：高矮、高低”，假若将无序属性连续化，则会不恰当地引入序关系，对后续处理如距离计算等造成误导==

回归任务中**最常用的性能度量：均方误差（最小化）**
均方误差具有非常好的几何意义，对应了常用的欧氏距离，利用均方误差最小化来进行模型求解的方法称为**最小二乘法（least square method）**

求解w与b的过程：**线性回归的参数估计（parameter estimation）**

当输入为多个属性时，则为**多元线性回归multivariate linear regression**


在多元线性回归中，为了便于讨论，我们表示为矩阵形式：

$$
X=\left\{
\begin{matrix}
x_{11}&x_{12}&\dots&x_{1d}&1\\
x_{21}&x_{22}&\dots&x_{2d}&1\\
\vdots&\vdots&\ddots&\vdots&\vdots\\ 
x_{m1}&x_{m2}&\dots&x_{md}&1
\end{matrix}
\right\}
\displaystyle \^w =(w;b)
$$

欲求得的结果为：
$\it\displaystyle \^w^*=\arg \mathop{\min}\limits_{\^w}(y-X\^w)^T(y-X\^w)$

只需对$\^w$求导即可求得最优解

$X^TX$为满秩矩阵时，求得：
$\^w^*=(X^TX)^{-1}X^Ty$

线性回归模型则为：
$f(\^x_i=\^x_i(X^TX)^{-1}X^Ty)$

事实上，并非满秩矩阵，属性数目远多于样例数目，则会解出==多个解==，选择哪一个解作为输出，将有学习算法的归纳偏好决定，常见的做法是**引入正则项（regularization）**。

### 广义线性模型

#### 对数线性模型
实际上是在**输入空间到输出空间的非线性映射**
$y=e^{w^Tx+b}$
但是lny与x的关系是线性映射，他们的线性映射也可以反映x与y的非线性映射

==更一般地==，$y=g^{-1}(w^Tx+b)$为广义线性模型（generalized linear model），其中，$g^{-1}$称为联系函数（link function）,这个联系函数可以将单调递增的函数转化为线性映射

可以通过求解w与b求得广义线性模型来求非线性映射，简化了求解一个非线性映射的步骤

### 对数几率回归
分类学习中的应用：
**需要将分类标记（0,1之类）与回归模型的预测值联系起来**

最理想的是单位阶跃函数（unit-step function）
$y=\begin{cases}
0,z<0;\\
0.5,z=0;\\
1，z>0
\end{cases}$

**但是，单位阶跃函数不连续，因此不能作为$g^{-1}(.)$，于是，我们希望求得替代函数，并希望它单调可微**

对数几率函数便是这样的函数
$y=\frac{1}{1+e^{-z}}$

对数几率函数是一种"Sigmoid函数"

对数几率变化为$\ln\frac{y}{1-y}=w^Tx+b$

将其中的y视为样本作为正例的可能性，则1-y作为x作为负例的可能性，$\frac{y}{1-y}$反映了x作为正例的相对可能性，称为几率，$\ln\frac{y}{1-y}$
称为对数几率（log odds，亦称logit

==所以，对数几率回归就是用用线性回归模型的预测结果来逼近对数几率，虽然名为回归，但实质上是一种分类方法==

对数几率回归的优点：
- 直接对分类可能性（几率）进行建模而不需要事先假设数据分布，这样就避免了假设分布不准确带来的问题
- 不是仅预测出类别，而是可得到近似概率预测，对许多需要利用概率辅助决策的任务很有用
- 对数几率函数是任意阶可导的函数，有很好的数学性质


极大似然法求解w、b
- 令每个样本属于其真实标记的概率越大越好(也就相当于求出一个最符合数据真实情况的模型)
$l(w,b)=\sum^m_{i=1}lnp(y_i|x_i;w,b)$

- 其中的似然项:$p(y_i|x_i;w,b)=y_ip1(\^x_i;\beta)+(1-y_i)p0(\^x_i;\beta)$

- 极大似然法的目标函数可转化为
$l(\beta)=\sum^{m}_{i=1}(-y_i\beta^T\^x_i+ln(1+e^{\beta^T\^x_i}))$

- 极大似然法转化为求此式的最小值，由于该式为关于$\beta$高阶可导的连续凸函数，可以使用许多数值优化算法如梯度下降法、牛顿法等来计算最优解，于是得到
$\beta^*=\arg \mathop{min}\limits_\beta l(\beta)$
$\beta=(w;b)
\^x=(x;1)
$

### 线性判别分析(Linear Discriminant Analysis)LDA
是一种经典的线性学习方法

**思想：将样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例投影点尽可能远离，在对新样本分类时，看他在这条直线上的投影点的位置**

**令同类样本投影点尽可能近，异类尽可能远**：
$w\Sigma w^T$尽可能小

$||w^T\mu_0-w^T\mu_1||^2_2$尽可能小

同时考虑两者，得到最大化的目标：
$J=\frac{||w^T\mu_0-w^T\mu_1||_2^2}{w\Sigma_0 w^T+w\Sigma_1 w^T}\\=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}$

类内散度矩阵
$S_w=\Sigma_0+\Sigma_1$

类间散度矩阵
$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$

将这两个矩阵代入J
$J=\frac{w^TS_bw}{w^TS_ww}$

这是$S_b$与$S_w$的广义瑞利商

当两类数据同先验，满足高斯分布且协方差相等时，LDA可以达到最优分类
### 多分类学习
**一些二分类算法可以直接推广到多分类算法，但更多时候是利用二分类学习器来解决多分类问题**

基本思路：拆解法

- 将多分类问题拆分为若干个二分类任务求解
- 先对问题进行拆分
- 再对拆出的每个二分类任务训练一个分类器
- 测试时，对这些分类的预测结果进行集成已获得最终的多分类结果

最经典的拆分策略：
一对一ovo
一对其余ovr
多对多mvm

#### 一对一OvO：
将N个类别两两配对，产生N(N-1)/2个二分类任务，产生N(N-1)/2个分类器，新样本将提交给所有的分类器，得到N(N-1)/2个结国，最终通过投票产生最终结果

#### 一对其余OvR：
每次将一类作为正例，其余类作为反例，
产生N个分类器
测试时，若仅有一个分类器判断为正例，则为正类，若有多个分类器判断为正类，则通常考虑分类器的预测置信度，预测置信度大的类别作为标记

OvO与OvR相比，在类别很多时，OvO比OvR的训练开销小，其他时候反之，预测性能取决于数据分布，大多数时候差不多
#### 多对多MvM
将若干类视为正类，若干类视为反类

纠错输出码（ECOC）为特殊的一类

### 类别不平衡
**不同类别的数目若差别很大，会对学习过程进行严重干扰**

由于我们通常假设训练集是真实样本的无偏采样，因此观测概率代表了真实概率，分类器预测大于观测几率就可以判断为正例

解决类别不平衡问题的方法：
- 欠采样：丢弃一些负例
- 过采样：增加一些正例
- 阈值移动：采用再缩放改变观察指标

欠采样可能会丢失信息
过采样不能简单对初始样例样本进行重复采样，否则会过拟合