# 支持向量机SVM
**基本思想：找到一个==最佳的==超平面，将样本空间中不同类别的样本区分开。**
前置概念：
支持向量：距离超平面最近的几个训练样本使得$\begin{cases}w^Tx_1+b\geq +1,y_i=+1\\w^Tx_i+b\leq-1,y_i=-1\end{cases}$中的等号成立
间隔：两个异类支持向量到超平面距离的和

### 策略：
找到最大间隔的划分超平面，也即找到最佳的w、b，使得
$\mathop{max}\limits_{w,b}\frac{2}{||w||}\\s.t.~y_i(w^Tx_i+b)\geq1,i=1,2,3……,m$

第一步转换:
将问题转化为：
$\mathop{min}\limits_{w,b}\frac{1}{2}||w||^2\\s.t.~~y_i(w^Tx_i+b)\geq1,i=1,2,3……,m$
此为支持向量机的基本型

### 求解基本型
#### 利用凸二次规划的优化计算包求解
#### 转为对偶问题求解
$max \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1} \sum^m_{j=1}\alpha_i \alpha_j y_i y_jx_i^Tx_j 
\\
s.t.\sum^m_{i=1}\alpha_iy_i=0\\\alpha_i\geq0,i=1,2,3,……,m$

由kkt条件可知，最终模型仅与支持向量有关。

利用SMO算法进一步求解。

### 核函数
前面讨论的是寻找一个最佳超平面能将训练样本正确划分，但是在现实任务中，原始样本空间也许并不存在一个正确划分两类样本的空间。

此时可以将原始空间映射到更高维的特征空间，使得在这个特征空间中能够线性可分。==幸运的是，若原始空间有限，即属性数有限，那么一定存在一个高维特征空间可以使样本可分。==

$令\phi(x)表示x的映射后的特征向量，映射后与前面讨论的情形相同，求解对偶问题：\\\mathop{max}\limits_{\alpha}\sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_{i}\alpha_{j} y_{i}y_{j} \phi(x_i)^T\phi(x_j)\\s.t.~~\sum^m_{i=1}\alpha_iy_j=0\\\alpha_i\geq0,i=1,2,3,……,m$

但是实际上特征空间维数可能很高，甚至可能是无穷维的，因此直接计算$\phi(x_i)^T\phi(x_j)$会困难一些，所以引入核函数(kernel function)来简化问题:
$\kappa(x_i,x_j)=<\phi(x_i),\phi(x_j)>=\phi(x_i)^T\phi(x_j)$

f(x)通过核函数展开:$f(x)=\sum^m_{i=1}\alpha_iy_i\kappa(x,x_i)+b$称为支持向量展式(support vector expansion)

定理:6.1:
对于一个半正定矩阵，总能找到一个与之对应的映射，换言之，任何一个核函数都隐式地定义为一个再生核希尔伯特空间RKHS

==特征空间的好坏对支持向量机的性能至关重要，核函数的选择成了最大变数==

核函数还可以通过线性组合得到、直积、乘以其他函数得到

### 软间隔与正则化
软间隔：允许支持向量机在一些样本上出错。以此缓解过拟合及增强鲁棒性。

